{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83ec8c89",
   "metadata": {},
   "source": [
    "# Machine Learning Assignment - 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357b89ee",
   "metadata": {},
   "source": [
    "# 1. Define the Bayesian interpretation of probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1453462a",
   "metadata": {},
   "source": [
    "The Bayesian interpretation of probability is a philosophical approach that views probability as a measure of belief or subjective uncertainty. According to this interpretation, probabilities represent our degree of confidence or rational belief in a particular event or hypothesis. It is based on Bayes' theorem, which allows us to update our beliefs in the light of new evidence.\n",
    "\n",
    "Let's go through a step-by-step example to illustrate the Bayesian interpretation of probability:\n",
    "\n",
    "Step 1: Initial Prior Probability\n",
    "We start with an initial belief or prior probability before any evidence is considered. For example, let's say we want to determine the probability that a patient has a certain disease. Our initial belief, based on general knowledge or prevalence rates, could be that 10% of the population has the disease. So, we assign a prior probability of 0.1 (or 10%) to the event \"patient has the disease.\"\n",
    "\n",
    "Step 2: Gathering Evidence\n",
    "Next, we gather evidence related to the event or hypothesis we are interested in. Continuing with our example, we conduct a medical test to determine whether the patient has the disease. The test has a known accuracy rate of 95% for correctly identifying positive cases (sensitivity) and 90% for correctly identifying negative cases (specificity).\n",
    "\n",
    "Step 3: Likelihood Function\n",
    "We calculate the likelihood of the evidence given the hypothesis. In our case, we calculate the probability of obtaining the test result we observed (positive or negative) if the patient has the disease or does not have the disease. This is done using the sensitivity and specificity rates of the test. Let's say the patient tests positive for the disease.\n",
    "\n",
    "Step 4: Applying Bayes' Theorem\n",
    "Using Bayes' theorem, we update our prior probability with the likelihood of the evidence to obtain the posterior probability. Bayes' theorem states:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "where:\n",
    "\n",
    "P(A|B) is the posterior probability (probability of hypothesis A given evidence B)\n",
    "P(B|A) is the likelihood of evidence B given hypothesis A\n",
    "P(A) is the prior probability of hypothesis A\n",
    "P(B) is the probability of evidence B\n",
    "In our example, we want to calculate the posterior probability that the patient has the disease (A) given that the test result is positive (B).\n",
    "\n",
    "Step 5: Calculating the Posterior Probability\n",
    "Using the values from our example, let's say the sensitivity of the test is 0.95 (95%) and the prior probability of having the disease is 0.1 (10%). The specificity is 0.90 (90%), and the probability of a positive test result for a non-diseased person is 0.10 (10%). The probability of a positive test result (P(B)) can be calculated using the law of total probability:\n",
    "\n",
    "P(B) = P(B|A) * P(A) + P(B|~A) * P(~A)\n",
    "\n",
    "P(B) = 0.95 * 0.1 + 0.10 * 0.9\n",
    "P(B) = 0.095 + 0.09\n",
    "P(B) = 0.185\n",
    "\n",
    "Now we can use Bayes' theorem to calculate the posterior probability:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "P(A|B) = (0.95 * 0.1) / 0.185\n",
    "P(A|B) ≈ 0.514 (approximately 51.4%)\n",
    "\n",
    "Therefore, based on the positive test result, the posterior probability that the patient has the disease is approximately 51.4%.\n",
    "\n",
    "Step 6: Updating the Belief\n",
    "The posterior probability becomes the new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5758bb2c",
   "metadata": {},
   "source": [
    "# 2. Define probability of a union of two events with equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2adf18",
   "metadata": {},
   "source": [
    "The probability of the union of two events, denoted as P(A ∪ B), represents the probability that either event A or event B (or both) will occur. It can be calculated using the following equation:\n",
    "\n",
    "P(A ∪ B) = P(A) + P(B) - P(A ∩ B)\n",
    "\n",
    "Let's walk through an example step by step to illustrate how to calculate the probability of the union of two events:\n",
    "\n",
    "Step 1: Determine the Individual Probabilities\n",
    "First, you need to determine the probabilities of the individual events, P(A) and P(B). Let's consider a bag of colored marbles. Event A is drawing a red marble, and event B is drawing a blue marble. Suppose the probability of drawing a red marble, P(A), is 0.4, and the probability of drawing a blue marble, P(B), is 0.3.\n",
    "\n",
    "Step 2: Determine the Probability of the Intersection\n",
    "Next, you need to determine the probability of the intersection of the two events, P(A ∩ B). This represents the probability that both events A and B occur simultaneously. In our example, let's say the probability of drawing a red marble and a blue marble simultaneously is 0.1.\n",
    "\n",
    "Step 3: Apply the Formula\n",
    "Using the formula for the probability of the union, we can calculate P(A ∪ B):\n",
    "\n",
    "P(A ∪ B) = P(A) + P(B) - P(A ∩ B)\n",
    "= 0.4 + 0.3 - 0.1\n",
    "= 0.6\n",
    "\n",
    "Therefore, the probability of drawing either a red marble or a blue marble (or both) from the bag is 0.6, or 60%.\n",
    "\n",
    "In this example, we calculated the probability of the union of two events using the formula P(A ∪ B) = P(A) + P(B) - P(A ∩ B). It's important to subtract the probability of the intersection (P(A ∩ B)) to avoid double-counting cases where both events occur simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5541ec",
   "metadata": {},
   "source": [
    "# 3. What is joint probability? What is its formula?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0989a39",
   "metadata": {},
   "source": [
    "Joint probability refers to the probability of two or more events occurring simultaneously. It measures the likelihood of the intersection or overlap between events. The joint probability of two events, A and B, is denoted as P(A and B) or P(A ∩ B).\n",
    "\n",
    "The formula for calculating the joint probability depends on whether the events are independent or dependent.\n",
    "\n",
    "Independent Events:\n",
    "If events A and B are independent, meaning that the occurrence of one event does not affect the probability of the other event, the joint probability is calculated as the product of their individual probabilities:\n",
    "P(A and B) = P(A) * P(B)\n",
    "\n",
    "For example, if you toss a fair coin twice, the probability of getting heads (event A) on the first toss and heads (event B) on the second toss would be:\n",
    "\n",
    "P(A and B) = P(A) * P(B) = 0.5 * 0.5 = 0.25\n",
    "\n",
    "Dependent Events:\n",
    "If events A and B are dependent, meaning that the occurrence of one event affects the probability of the other event, the joint probability is calculated using conditional probability:\n",
    "P(A and B) = P(A | B) * P(B)\n",
    "\n",
    "Here, P(A | B) represents the conditional probability of event A occurring given that event B has occurred.\n",
    "\n",
    "For example, consider drawing two cards from a standard deck without replacement. Let event A be drawing a red card on the first draw, and event B be drawing a red card on the second draw. The joint probability can be calculated as follows:\n",
    "\n",
    "P(A and B) = P(A | B) * P(B)\n",
    "= (26/51) * (25/50)\n",
    "= 13/51\n",
    "\n",
    "In this case, the joint probability of drawing a red card on both the first and second draws is 13/51.\n",
    "\n",
    "Note that the formula for joint probability can be extended to more than two events by multiplying the individual probabilities or conditional probabilities as appropriate, depending on whether the events are independent or dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a66a0c",
   "metadata": {},
   "source": [
    "# 4. What is chain rule of probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6478d29",
   "metadata": {},
   "source": [
    "The chain rule of probability, also known as the multiplication rule, is a fundamental principle in probability theory that allows us to calculate the probability of the intersection or joint occurrence of multiple events. It states that the joint probability of multiple events can be expressed as the product of the conditional probabilities of each event given the occurrence of the preceding events in the sequence.\n",
    "\n",
    "Mathematically, the chain rule of probability can be stated as follows:\n",
    "\n",
    "P(A₁, A₂, A₃, ..., Aₙ) = P(A₁) * P(A₂ | A₁) * P(A₃ | A₁, A₂) * ... * P(Aₙ | A₁, A₂, ..., Aₙ₋₁)\n",
    "\n",
    "In this formula, P(A₁, A₂, A₃, ..., Aₙ) represents the joint probability of events A₁, A₂, A₃, ..., Aₙ occurring together.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82809de",
   "metadata": {},
   "source": [
    "# 5. What is conditional probability means? What is the formula of it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac14b6c",
   "metadata": {},
   "source": [
    "Conditional probability is a measure of the probability of an event occurring given that another event has already occurred. It quantifies the likelihood of an event A happening, taking into account the knowledge or information that event B has occurred. It is denoted as P(A|B), read as \"the probability of A given B.\"\n",
    "\n",
    "The formula for conditional probability is as follows:\n",
    "\n",
    "P(A|B) = P(A ∩ B) / P(B)\n",
    "\n",
    "In this formula:\n",
    "\n",
    "P(A|B) represents the conditional probability of event A occurring given that event B has occurred.\n",
    "P(A ∩ B) represents the joint probability of events A and B occurring together.\n",
    "P(B) represents the probability of event B occurring.\n",
    "To calculate the conditional probability, you divide the joint probability of events A and B by the probability of event B.\n",
    "\n",
    "Let's go through an example to illustrate the calculation of conditional probability:\n",
    "\n",
    "Suppose you have a deck of cards, and you draw one card at random. Event A is drawing a red card, and event B is drawing a heart (one specific suit). Let's calculate the probability of drawing a heart (event B) given that you have already drawn a red card (event A).\n",
    "\n",
    "P(A ∩ B) is the joint probability of drawing a red heart card. There are two red hearts in the deck, so P(A ∩ B) = 2/52 = 1/26.\n",
    "\n",
    "P(A) is the probability of drawing a red card, and there are 26 red cards in the deck (including the two red hearts). So, P(A) = 26/52 = 1/2.\n",
    "\n",
    "Now, we can calculate the conditional probability using the formula:\n",
    "\n",
    "P(B|A) = P(A ∩ B) / P(A)\n",
    "= (1/26) / (1/2)\n",
    "= 1/13\n",
    "\n",
    "Therefore, the probability of drawing a heart (event B) given that you have already drawn a red card (event A) is 1/13.\n",
    "\n",
    "Conditional probability allows us to update our probabilities or make more accurate predictions based on the information we have about related events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3092ee",
   "metadata": {},
   "source": [
    "# 6. What are continuous random variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205e1d11",
   "metadata": {},
   "source": [
    "Continuous random variables are variables in probability theory that can take on any value within a specific range or interval. Unlike discrete random variables, which can only take on distinct, separate values, continuous random variables can assume any value within a continuous range. They are typically associated with measurements and quantities that can be expressed as real numbers.\n",
    "\n",
    "Examples of continuous random variables include:\n",
    "\n",
    "Height: The height of a person can take on any value within a range, such as between 150 cm and 200 cm. It can be measured with arbitrary precision, allowing for decimal values.\n",
    "\n",
    "Time: The time it takes for a process to complete or the arrival time of an event can be considered as a continuous random variable. It can be measured with increasing precision, such as milliseconds, microseconds, or even nanoseconds.\n",
    "\n",
    "Temperature: The temperature measured in degrees Celsius or Fahrenheit can be treated as a continuous random variable. It can take on any value within a range, allowing for decimal values.\n",
    "\n",
    "Continuous random variables are typically described using probability density functions (PDFs) instead of probability mass functions (PMFs). A PDF represents the probability distribution over the range of possible values of the random variable. The area under the PDF curve within a given interval represents the probability of the random variable falling within that interval.\n",
    "\n",
    "Since continuous random variables have an infinite number of possible values, the probability of observing any exact value is typically zero. Instead, probabilities are defined for intervals or ranges of values.\n",
    "\n",
    "Probability calculations involving continuous random variables often require integration, such as finding the probability of the random variable falling within a specific range or calculating expected values.\n",
    "\n",
    "Examples of probability distributions associated with continuous random variables include the normal distribution, uniform distribution, exponential distribution, and beta distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aceba3",
   "metadata": {},
   "source": [
    "# 7. What are Bernoulli distributions? What is the formula of it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab474f8",
   "metadata": {},
   "source": [
    "The Bernoulli distribution is a discrete probability distribution that models a random experiment with two possible outcomes: success and failure. It is named after Jacob Bernoulli, a Swiss mathematician. The outcomes are often denoted as 1 for success and 0 for failure.\n",
    "\n",
    "The probability mass function (PMF) of the Bernoulli distribution is given by the following formula:\n",
    "\n",
    "P(X = k) = p^k * (1 - p)^(1-k)\n",
    "\n",
    "In this formula:\n",
    "\n",
    "P(X = k) represents the probability of the random variable X taking the value k.\n",
    "p is the probability of success. It must be a value between 0 and 1.\n",
    "k is the outcome of the experiment, which can only take on values 0 or 1.\n",
    "The PMF of the Bernoulli distribution states that the probability of success, p, is raised to the power of k, and the probability of failure, (1 - p), is raised to the power of (1 - k).\n",
    "\n",
    "Properties of the Bernoulli distribution:\n",
    "\n",
    "Mean (expected value): E(X) = p\n",
    "The mean of a Bernoulli distribution is equal to the probability of success, p.\n",
    "\n",
    "Variance: Var(X) = p * (1 - p)\n",
    "The variance of a Bernoulli distribution is equal to the product of the probability of success, p, and the probability of failure, (1 - p).\n",
    "\n",
    "The Bernoulli distribution is often used as a building block for more complex distributions and models. It serves as the foundation for other important distributions, such as the binomial distribution, which deals with the number of successes in a fixed number of independent Bernoulli trials.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2794f7a2",
   "metadata": {},
   "source": [
    "# 8. What is binomial distribution? What is the formula?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc14d1a",
   "metadata": {},
   "source": [
    "The binomial distribution is a discrete probability distribution that models the number of successes in a fixed number of independent Bernoulli trials. It is named after the \"binomial theorem\" from combinatorics. The trials are characterized by two possible outcomes: success and failure, which are often denoted as 1 and 0, respectively.\n",
    "\n",
    "The formula for the probability mass function (PMF) of the binomial distribution is:\n",
    "\n",
    "P(X = k) = C(n, k) * p^k * (1 - p)^(n - k)\n",
    "\n",
    "In this formula:\n",
    "\n",
    "P(X = k) represents the probability of obtaining k successes in n trials.\n",
    "C(n, k) is the binomial coefficient, which represents the number of ways to choose k successes from n trials and is calculated as C(n, k) = n! / (k! * (n - k)!), where \"!\" denotes the factorial.\n",
    "p is the probability of success in a single trial. It must be a value between 0 and 1.\n",
    "k is the number of successes, ranging from 0 to n, the total number of trials.\n",
    "Properties of the binomial distribution:\n",
    "\n",
    "Mean (expected value): E(X) = n * p\n",
    "The mean of a binomial distribution is equal to the product of the number of trials, n, and the probability of success, p.\n",
    "\n",
    "Variance: Var(X) = n * p * (1 - p)\n",
    "The variance of a binomial distribution is equal to the product of the number of trials, n, the probability of success, p, and the probability of failure, (1 - p).\n",
    "\n",
    "The binomial distribution is commonly used to model various real-world scenarios, such as flipping a coin multiple times, conducting opinion polls, or examining the success rates of experiments with binary outcomes. It provides a framework for analyzing and understanding the probability of different numbers of successes occurring in a fixed number of independent trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83481f95",
   "metadata": {},
   "source": [
    "# 9. What is Poisson distribution? What is the formula?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809becc",
   "metadata": {},
   "source": [
    "The Poisson distribution is a discrete probability distribution that models the number of events that occur within a fixed interval of time or space. It is named after the French mathematician Siméon Denis Poisson.\n",
    "\n",
    "The Poisson distribution is typically used when the events occur randomly and independently of each other, and the average rate of occurrence is known or estimated.\n",
    "\n",
    "The formula for the probability mass function (PMF) of the Poisson distribution is:\n",
    "\n",
    "P(X = k) = (e^(-λ) * λ^k) / k!\n",
    "\n",
    "In this formula:\n",
    "\n",
    "P(X = k) represents the probability of observing k events.\n",
    "e is the base of the natural logarithm, approximately equal to 2.71828.\n",
    "λ (lambda) is the average rate of events occurring in the given interval. It is also equal to the mean and variance of the Poisson distribution.\n",
    "k is the number of events, which can take any non-negative integer value.\n",
    "Properties of the Poisson distribution:\n",
    "\n",
    "Mean (expected value) and variance: E(X) = Var(X) = λ\n",
    "The mean and variance of a Poisson distribution are both equal to λ, the average rate of events.\n",
    "\n",
    "Memorylessness: The Poisson distribution is memoryless, meaning that the probability of an event occurring in the future is not affected by the past. In other words, the time since the last event does not impact the probability of the next event occurring.\n",
    "\n",
    "The Poisson distribution is commonly used to model the number of occurrences of rare events, such as the number of phone calls received at a call center in a given time interval, the number of emails received per hour, or the number of accidents at an intersection in a day. It provides a way to estimate the likelihood of different counts of events within a specified interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f8480f",
   "metadata": {},
   "source": [
    "# 10. Define covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8aec50",
   "metadata": {},
   "source": [
    "Covariance is a measure of the relationship between two random variables. It quantifies how changes in one variable correspond to changes in another variable. Specifically, covariance measures the extent to which two variables move together or apart from their respective means.\n",
    "\n",
    "Mathematically, the covariance between two random variables X and Y is denoted as Cov(X, Y). The formula for covariance is:\n",
    "\n",
    "Cov(X, Y) = E[(X - E(X)) * (Y - E(Y))]\n",
    "\n",
    "In this formula:\n",
    "\n",
    "X and Y represent the random variables.\n",
    "E(X) and E(Y) represent the expected values (means) of X and Y, respectively.\n",
    "The formula calculates the average of the product of the deviations of X and Y from their means. If the product is positive, it indicates that X and Y tend to have similar deviations from their means, suggesting a positive relationship. If the product is negative, it indicates that X and Y tend to have opposite deviations from their means, suggesting a negative relationship. A covariance of zero suggests no linear relationship between the variables.\n",
    "\n",
    "Properties of covariance:\n",
    "\n",
    "Covariance is a measure of association, not scale. It is affected by the units of measurement of the variables.\n",
    "Covariance is symmetric, meaning Cov(X, Y) = Cov(Y, X).\n",
    "The magnitude of covariance does not have a specific range. It can take any value, positive or negative, depending on the relationship between the variables.\n",
    "Covariance is often used to assess the direction and strength of the linear relationship between two variables. However, it does not provide a standardized measure of association, as its value is influenced by the scales of the variables. To overcome this limitation, the concept of correlation is used, which is a standardized measure of the linear relationship between variables obtained by dividing the covariance by the product of their standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d524805",
   "metadata": {},
   "source": [
    "# 11. Define correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b67bef",
   "metadata": {},
   "source": [
    "Correlation is a statistical measure that quantifies the strength and direction of the linear relationship between two variables. It provides information about how closely the variables are related and whether they move together or in opposite directions.\n",
    "\n",
    "The correlation coefficient, typically denoted as \"r,\" is used to represent the correlation between two variables. The correlation coefficient can take values between -1 and +1, where:\n",
    "\n",
    "A correlation coefficient of +1 indicates a perfect positive correlation, meaning that the variables have a strong linear relationship and move together in the same direction.\n",
    "A correlation coefficient of -1 indicates a perfect negative correlation, meaning that the variables have a strong linear relationship but move in opposite directions.\n",
    "A correlation coefficient of 0 indicates no linear relationship between the variables.\n",
    "The formula for calculating the correlation coefficient, assuming we have two variables X and Y with n data points, is:\n",
    "\n",
    "r = (Σ((X - μX) * (Y - μY))) / (√(Σ(X - μX)^2) * √(Σ(Y - μY)^2))\n",
    "\n",
    "In this formula:\n",
    "\n",
    "X and Y represent the variables.\n",
    "μX and μY represent the means (expected values) of X and Y, respectively.\n",
    "Σ denotes the summation operator, which calculates the sum over the given range.\n",
    "The correlation coefficient measures the strength and direction of the linear relationship between the variables. The closer the correlation coefficient is to +1 or -1, the stronger the linear relationship. A correlation coefficient of 0 indicates no linear relationship.\n",
    "\n",
    "It's important to note that correlation measures the linear relationship between variables, and it does not imply causation. Correlation only assesses how two variables move together or apart, but it does not indicate that one variable causes the other to change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8d7f36",
   "metadata": {},
   "source": [
    "# 12. Define sampling with replacement. Give example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df912c6",
   "metadata": {},
   "source": [
    "Sampling with replacement is a method of selecting items or individuals from a population, where each selection is independent and does not affect the availability of the selected item for future selections. After each selection, the selected item is returned to the population, making it available for selection again in subsequent draws.\n",
    "\n",
    "Here's an example to illustrate sampling with replacement:\n",
    "\n",
    "Suppose you have a bag with five colored balls: red, blue, green, yellow, and orange. You want to randomly select three balls from the bag using sampling with replacement.\n",
    "\n",
    "Initially, the bag contains five balls: {red, blue, green, yellow, orange}.\n",
    "You randomly select the first ball and record its color. Let's say you pick the blue ball.\n",
    "After recording the color, you return the blue ball to the bag, so the bag still contains five balls: {red, blue, green, yellow, orange}.\n",
    "You mix the balls in the bag to ensure randomness.\n",
    "You repeat the process and randomly select the second ball. Let's say you pick the red ball.\n",
    "Again, you return the red ball to the bag, so the bag still contains five balls: {red, blue, green, yellow, orange}.\n",
    "You mix the balls in the bag.\n",
    "Finally, you randomly select the third ball. Let's say you pick the green ball.\n",
    "In this example, you performed sampling with replacement. Each time you selected a ball, you returned it to the bag before making the next selection. This process ensures that each ball has an equal chance of being selected at each draw, regardless of previous selections.\n",
    "\n",
    "Sampling with replacement allows for the possibility of selecting the same item multiple times in a single sample. It is commonly used in scenarios where the population size is large, and the impact of each selection on the overall population is negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2ed394",
   "metadata": {},
   "source": [
    "# 13. What is sampling without replacement? Give example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd0dc3",
   "metadata": {},
   "source": [
    "Sampling without replacement is a method of selecting items or individuals from a population in which each selection removes the chosen item from the population, making it unavailable for future selections. Once an item is selected, it is not returned to the population for subsequent draws.\n",
    "\n",
    "Here's an example to illustrate sampling without replacement:\n",
    "\n",
    "Suppose you have a deck of playing cards containing 52 cards. You want to randomly select three cards from the deck using sampling without replacement.\n",
    "\n",
    "Initially, the deck contains 52 cards: {Ace of Spades, 2 of Hearts, 3 of Diamonds, ..., King of Clubs}.\n",
    "You randomly select the first card and record its value and suit. Let's say you pick the 5 of Diamonds.\n",
    "After recording the card, you remove it from the deck, so the deck now contains 51 cards: {Ace of Spades, 2 of Hearts, 3 of Diamonds, ..., King of Clubs} (without the 5 of Diamonds).\n",
    "You mix the remaining cards in the deck to ensure randomness.\n",
    "You repeat the process and randomly select the second card. Let's say you pick the King of Hearts.\n",
    "Similarly, you remove the King of Hearts from the deck, so the deck now contains 50 cards: {Ace of Spades, 2 of Hearts, 3 of Diamonds, ..., King of Clubs} (without the 5 of Diamonds and the King of Hearts).\n",
    "You mix the remaining cards in the deck.\n",
    "Finally, you randomly select the third card. Let's say you pick the 9 of Clubs.\n",
    "In this example, you performed sampling without replacement. Each time you selected a card, it was removed from the deck, making it unavailable for future selections. This ensures that each card can only be selected once in the sample.\n",
    "\n",
    "Sampling without replacement is commonly used when it is necessary to ensure that each item or individual in the population has an equal chance of being selected exactly once. It is often employed in surveys, experiments, and statistical analyses where the removal of selected items is required to maintain the integrity of the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5924bdad",
   "metadata": {},
   "source": [
    "# 14. What is hypothesis? Give example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb05862",
   "metadata": {},
   "source": [
    "In statistics and scientific research, a hypothesis is a proposed explanation or prediction about a phenomenon or relationship between variables. It is a statement that can be tested and evaluated based on empirical evidence.\n",
    "\n",
    "Hypotheses are typically formulated as two opposing statements: the null hypothesis (H₀) and the alternative hypothesis (H₁). The null hypothesis states that there is no significant effect or relationship, while the alternative hypothesis suggests that there is a significant effect or relationship.\n",
    "\n",
    "Here's an example to illustrate a hypothesis:\n",
    "\n",
    "Let's say a researcher is studying the effect of a new drug on reducing blood pressure. The researcher might formulate the following hypotheses:\n",
    "\n",
    "Null Hypothesis (H₀): The new drug has no significant effect on reducing blood pressure.\n",
    "Alternative Hypothesis (H₁): The new drug has a significant effect on reducing blood pressure.\n",
    "\n",
    "To test these hypotheses, the researcher would design a study where participants are randomly assigned to two groups: one group receiving the new drug (treatment group) and another group receiving a placebo or standard treatment (control group). The researcher would measure the participants' blood pressure before and after the treatment.\n",
    "\n",
    "After collecting the data, statistical analysis techniques can be employed to determine if there is sufficient evidence to support or reject the null hypothesis in favor of the alternative hypothesis. The analysis might involve comparing the average blood pressure reduction between the treatment and control groups using appropriate statistical tests.\n",
    "\n",
    "If the statistical analysis yields strong evidence of a significant reduction in blood pressure in the treatment group compared to the control group, the researcher may reject the null hypothesis and conclude that the new drug has a significant effect on reducing blood pressure.\n",
    "\n",
    "Hypotheses are essential in scientific research as they guide the formulation of research questions, the design of studies, and the interpretation of results. They allow researchers to make informed predictions and draw meaningful conclusions based on empirical evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42e65c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f163cc2f",
   "metadata": {},
   "source": [
    "# 1. Provide an example of the concepts of Prior, Posterior, and Likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba4627b",
   "metadata": {},
   "source": [
    "Let's consider an example of a medical test for a rare disease. Assume the disease has a prevalence of 1% in the general population.\n",
    "\n",
    "Prior probability: The prior probability, denoted as P(D), represents the initial probability or belief of an event before any evidence is considered. In this case, the prior probability would be the probability of a randomly selected person having the disease without considering any test results. Given that the disease has a prevalence of 1% in the general population, the prior probability of having the disease would be P(D) = 0.01 or 1%.\n",
    "\n",
    "Likelihood probability: The likelihood probability, denoted as P(T|D), represents the probability of observing a particular test result given that the disease is present. In our example, let's say the medical test has a sensitivity of 95%, which means it correctly detects the disease 95% of the time when it is present. Therefore, the likelihood probability of a positive test result given that a person has the disease would be P(T|D) = 0.95 or 95%.\n",
    "\n",
    "Posterior probability: The posterior probability, denoted as P(D|T), represents the updated probability of an event after taking into account the evidence or test results. It is calculated using Bayes' theorem, which combines the prior probability and likelihood probability. In our example, we can calculate the posterior probability of a person having the disease given a positive test result using Bayes' theorem:\n",
    "\n",
    "P(D|T) = (P(T|D) * P(D)) / P(T)\n",
    "\n",
    "P(T) represents the probability of obtaining a positive test result, which can be calculated as:\n",
    "P(T) = P(T|D) * P(D) + P(T|~D) * P(~D)\n",
    "P(T|~D) represents the probability of a positive test result given that the person does not have the disease, and P(~D) represents the probability of not having the disease.\n",
    "\n",
    "Let's assume that the test has a specificity of 90%, meaning it correctly identifies negative results 90% of the time when the disease is absent. In this case:\n",
    "P(T|~D) = 1 - specificity = 1 - 0.90 = 0.10 or 10%\n",
    "P(~D) = 1 - P(D) = 1 - 0.01 = 0.99 or 99%\n",
    "\n",
    "Plugging in these values, we can calculate the posterior probability:\n",
    "P(D|T) = (0.95 * 0.01) / [(0.95 * 0.01) + (0.10 * 0.99)]\n",
    "= 0.087 / (0.087 + 0.099)\n",
    "≈ 0.467 or 46.7%\n",
    "\n",
    "So, given a positive test result, the posterior probability of a person actually having the disease is approximately 46.7%.\n",
    "\n",
    "In this example, the prior probability represents the initial probability of having the disease, the likelihood probability represents the probability of a positive test result given the disease, and the posterior probability represents the updated probability of having the disease after considering the test result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4851286",
   "metadata": {},
   "source": [
    "# 2. What role does Bayes&#39; theorem play in the concept learning principle?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec080ea",
   "metadata": {},
   "source": [
    "Bayes' theorem plays a fundamental role in the concept learning principle by providing a mathematical framework to update and revise beliefs or hypotheses based on new evidence or observations. The concept learning principle, often referred to as Bayesian inference or Bayesian learning, is based on Bayes' theorem and forms the foundation of probabilistic reasoning and learning.\n",
    "\n",
    "In the context of concept learning, Bayes' theorem allows us to update our beliefs about the probability of a hypothesis or concept being true given new evidence. It involves combining our prior beliefs (prior probability) with the likelihood of observing the evidence given the hypothesis (likelihood probability) to obtain the updated probability (posterior probability) of the hypothesis being true.\n",
    "\n",
    "The general form of Bayes' theorem is:\n",
    "\n",
    "P(H|E) = (P(E|H) * P(H)) / P(E)\n",
    "\n",
    "where:\n",
    "\n",
    "P(H|E) is the posterior probability of hypothesis H being true given evidence E.\n",
    "P(E|H) is the likelihood probability of observing evidence E given hypothesis H.\n",
    "P(H) is the prior probability of hypothesis H being true.\n",
    "P(E) is the probability of observing evidence E.\n",
    "In the concept learning process, the concept or hypothesis refers to a particular explanation or model that describes a set of observations or data. The evidence corresponds to the observed data or examples. Bayes' theorem allows us to update our beliefs about the likelihood of different hypotheses or concepts being true based on the observed evidence.\n",
    "\n",
    "By iteratively applying Bayes' theorem and updating the posterior probabilities, we can refine our understanding of the underlying concept or hypothesis. This iterative process of updating beliefs based on new evidence is a key principle in Bayesian learning and enables us to make more accurate predictions and inferences about the concept being learned.\n",
    "\n",
    "Overall, Bayes' theorem plays a central role in the concept learning principle by providing a principled and mathematically sound framework to update and revise beliefs based on new evidence, facilitating the process of learning and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a95d0",
   "metadata": {},
   "source": [
    "# 3. Offer an example of how the Nave Bayes classifier is used in real life."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef9b8c",
   "metadata": {},
   "source": [
    "One common real-life example of how the Naïve Bayes classifier is used is in email spam filtering. Email service providers and email clients often employ Naïve Bayes classifiers to automatically identify and classify incoming emails as either spam or legitimate (non-spam).\n",
    "\n",
    "Here's how the Naïve Bayes classifier is applied in email spam filtering:\n",
    "\n",
    "Training phase: During the training phase, a large labeled dataset of emails is used to train the Naïve Bayes classifier. Each email is manually labeled as spam or non-spam. The classifier learns the statistical patterns and relationships between the words or features in the emails and their corresponding labels.\n",
    "\n",
    "Feature extraction: Relevant features are extracted from the emails to represent their content. These features can include word frequencies, presence/absence of certain words, or more advanced representations like TF-IDF (Term Frequency-Inverse Document Frequency). The assumption of feature independence in Naïve Bayes means that each feature (word) is considered independently of others, simplifying the computations.\n",
    "\n",
    "Probability estimation: The Naïve Bayes classifier estimates the prior probabilities of spam and non-spam emails based on the training dataset. It also calculates the likelihood probabilities of the extracted features given each class (spam or non-spam) under the assumption of feature independence.\n",
    "\n",
    "Classification phase: Once the model is trained, it can be used to classify new, incoming emails. The classifier calculates the posterior probabilities of each class (spam or non-spam) given the observed features using Bayes' theorem. It assigns the email to the class with the highest posterior probability, categorizing it as either spam or non-spam.\n",
    "\n",
    "In real-life usage, the Naïve Bayes classifier for email spam filtering is continuously updated and refined with new labeled data to improve its performance and adapt to evolving spam patterns. It helps filter out unwanted emails, reducing the time and effort needed for users to manually sort through their inbox and protecting them from potential security risks associated with spam emails.\n",
    "\n",
    "The Naïve Bayes classifier's strengths, such as its simplicity, computational efficiency, and ability to handle high-dimensional feature spaces, make it an effective choice for email spam filtering applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf2a539",
   "metadata": {},
   "source": [
    "# 4. Can the Nave Bayes classifier be used on continuous numeric data? If so, how can you go about doing it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6977edb",
   "metadata": {},
   "source": [
    "Yes, the Naïve Bayes classifier can be used on continuous numeric data. However, it requires an assumption or approximation to handle the continuous nature of the data. There are a few common approaches to apply Naïve Bayes to continuous data:\n",
    "\n",
    "Binning or discretization: One approach is to convert the continuous numeric data into discrete categories or bins. This involves dividing the range of values into intervals or bins and then treating each bin as a separate categorical value. The Naïve Bayes classifier can then be applied to the discretized data using the standard approach.\n",
    "\n",
    "Gaussian assumption: Another common approach is to assume that the continuous numeric data follows a Gaussian (normal) distribution within each class. Under this assumption, the mean and standard deviation of each feature within each class can be estimated from the training data. During classification, the Naïve Bayes classifier calculates the probability of a particular value belonging to a class using the probability density function of the Gaussian distribution.\n",
    "\n",
    "Kernel density estimation: Instead of assuming a specific distribution, kernel density estimation can be used to estimate the probability density function of the continuous data within each class. This approach estimates the underlying probability density function based on the observed data and uses it to calculate the likelihood probabilities for classification.\n",
    "\n",
    "It's important to note that the Naïve Bayes classifier assumes independence between features. When working with continuous numeric data, one should consider the correlations or dependencies between the features. Techniques like principal component analysis (PCA) or feature selection methods can be used to address potential feature dependencies and reduce the dimensionality of the data.\n",
    "\n",
    "In summary, while the Naïve Bayes classifier is originally designed for discrete data, it can be adapted to handle continuous numeric data by discretizing the data, assuming a Gaussian distribution, or using kernel density estimation. The choice of approach depends on the nature of the data and the assumptions that can be made about its distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a8507b",
   "metadata": {},
   "source": [
    "# 5. What are Bayesian Belief Networks, and how do they work? What are their applications? Are they capable of resolving a wide range of issues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9723da",
   "metadata": {},
   "source": [
    "Bayesian Belief Networks (BBNs), also known as Bayesian Networks or Probabilistic Graphical Models, are graphical models that represent and reason about uncertain relationships between variables using probabilistic methods. They combine probability theory and graph theory to model the dependencies and conditional relationships between variables.\n",
    "\n",
    "Here's how Bayesian Belief Networks work:\n",
    "\n",
    "Structure: A Bayesian Belief Network consists of nodes (representing variables) and directed edges (representing dependencies) between the nodes. The structure of the network is typically represented as a directed acyclic graph (DAG), where each node corresponds to a variable and the edges represent probabilistic dependencies.\n",
    "\n",
    "Conditional probability tables: Each node in the Bayesian Belief Network has a conditional probability table (CPT) associated with it. The CPT specifies the conditional probabilities of a node given its parent nodes in the graph. It quantifies the uncertainty and probabilistic relationships between variables.\n",
    "\n",
    "Inference: Bayesian Belief Networks enable probabilistic inference, allowing for reasoning and computation of various probabilities of interest. Using the network structure and the conditional probability tables, inference algorithms can calculate probabilities of specific events or make predictions about unobserved variables given observed variables.\n",
    "\n",
    "Applications of Bayesian Belief Networks:\n",
    "\n",
    "Decision support systems: BBNs are used in decision support systems to model and analyze complex decision-making problems under uncertainty. They can help make optimal decisions by considering the probabilities and dependencies of various factors.\n",
    "\n",
    "Diagnosis and risk assessment: BBNs are utilized in medical diagnosis and risk assessment to model the relationships between symptoms, diseases, and risk factors. They can provide probabilistic predictions of diseases or identify the most likely causes based on observed symptoms.\n",
    "\n",
    "Predictive modeling: BBNs are employed in predictive modeling tasks, such as predicting customer behavior, fraud detection, or machine failure. They can incorporate various observed variables and their relationships to make probabilistic predictions about future events.\n",
    "\n",
    "Anomaly detection: BBNs can be used for anomaly detection by modeling normal behavior and identifying deviations from the expected patterns. They can help detect anomalies in network traffic, cybersecurity, or financial transactions.\n",
    "\n",
    "While Bayesian Belief Networks are powerful tools for probabilistic reasoning and have a wide range of applications, they do have limitations. They rely on accurate specification of the network structure and appropriate estimation of the conditional probabilities. Additionally, they may face challenges in handling large-scale networks and computationally expensive inference. However, with careful modeling and appropriate algorithms, Bayesian Belief Networks can effectively address a variety of issues involving uncertainty and dependencies between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd859e6",
   "metadata": {},
   "source": [
    "# 6. Passengers are checked in an airport screening system to see if there is an intruder. Let I be the\n",
    "random variable that indicates whether someone is an intruder I = 1) or not I = 0), and A be the\n",
    "variable that indicates alarm I = 0). If an intruder is detected with probability P(A = 1|I = 1) = 0.98\n",
    "and a non-intruder is detected with probability P(A = 1|I = 0) = 0.001, an alarm will be triggered,\n",
    "implying the error factor. The likelihood of an intruder in the passenger population is P(I = 1) =\n",
    "0.00001. What are the chances that an alarm would be triggered when an individual is actually an\n",
    "intruder?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9474e4",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93af82a8",
   "metadata": {},
   "source": [
    "To calculate the chances that an alarm would be triggered when an individual is actually an intruder, we can use Bayes' theorem.\n",
    "\n",
    "Let's denote:\n",
    "I = Random variable indicating whether someone is an intruder (I = 1) or not (I = 0).\n",
    "A = Random variable indicating whether an alarm is triggered (A = 1) or not (A = 0).\n",
    "\n",
    "We are given the following probabilities:\n",
    "P(A = 1|I = 1) = 0.98 (Probability of detecting an intruder correctly)\n",
    "P(A = 1|I = 0) = 0.001 (Probability of a false positive, detecting a non-intruder as an intruder)\n",
    "P(I = 1) = 0.00001 (Probability of an individual being an intruder)\n",
    "\n",
    "We want to calculate P(I = 1|A = 1), which is the probability that an individual is an intruder given that an alarm is triggered.\n",
    "\n",
    "Using Bayes' theorem, we have:\n",
    "\n",
    "P(I = 1|A = 1) = (P(A = 1|I = 1) * P(I = 1)) / P(A = 1)\n",
    "\n",
    "To calculate P(A = 1), we can use the law of total probability:\n",
    "P(A = 1) = P(A = 1|I = 1) * P(I = 1) + P(A = 1|I = 0) * P(I = 0)\n",
    "\n",
    "P(I = 0) = 1 - P(I = 1) = 1 - 0.00001 = 0.99999\n",
    "\n",
    "Plugging in the values, we have:\n",
    "P(A = 1) = (0.98 * 0.00001) + (0.001 * 0.99999)\n",
    "\n",
    "Now we can calculate P(I = 1|A = 1):\n",
    "P(I = 1|A = 1) = (0.98 * 0.00001) / [(0.98 * 0.00001) + (0.001 * 0.99999)]\n",
    "\n",
    "Simplifying the expression:\n",
    "P(I = 1|A = 1) = 0.0000098 / (0.0000098 + 0.00099999)\n",
    "\n",
    "Calculating the result:\n",
    "P(I = 1|A = 1) ≈ 0.0097 or 0.97%\n",
    "\n",
    "Therefore, the chances that an alarm would be triggered when an individual is actually an intruder is approximately 0.97%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52427129",
   "metadata": {},
   "source": [
    "# 7. An antibiotic resistance test (random variable T) has 1% false positives (i.e., 1% of those who are\n",
    "not immune to an antibiotic display a positive result in the test) and 5% false negatives (i.e., 1% of\n",
    "those who are not resistant to an antibiotic show a positive result in the test) (i.e. 5 percent of those\n",
    "actually resistant to an antibiotic test negative). Assume that 2% of those who were screened were\n",
    "antibiotic-resistant. Calculate the likelihood that a person who tests positive is actually immune\n",
    "(random variable D)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d36d1a",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323cb1f2",
   "metadata": {},
   "source": [
    "To calculate the likelihood that a person who tests positive is actually immune (D = 1), we can use Bayes' theorem.\n",
    "\n",
    "Let's denote:\n",
    "D = Random variable indicating whether a person is immune (D = 1) or not immune (D = 0) to an antibiotic.\n",
    "T = Random variable indicating the result of the antibiotic resistance test, where T = 1 represents a positive test result and T = 0 represents a negative test result.\n",
    "\n",
    "We are given the following probabilities:\n",
    "P(T = 1|D = 0) = 0.01 (Probability of a false positive, i.e., a positive test result for a person who is not immune)\n",
    "P(T = 0|D = 1) = 0.05 (Probability of a false negative, i.e., a negative test result for a person who is immune)\n",
    "P(D = 1) = 0.02 (Probability of a person being immune)\n",
    "\n",
    "We want to calculate P(D = 1|T = 1), which is the probability that a person is immune given a positive test result.\n",
    "\n",
    "Using Bayes' theorem, we have:\n",
    "\n",
    "P(D = 1|T = 1) = (P(T = 1|D = 1) * P(D = 1)) / P(T = 1)\n",
    "\n",
    "To calculate P(T = 1), we can use the law of total probability:\n",
    "P(T = 1) = P(T = 1|D = 1) * P(D = 1) + P(T = 1|D = 0) * P(D = 0)\n",
    "\n",
    "P(D = 0) = 1 - P(D = 1) = 1 - 0.02 = 0.98\n",
    "\n",
    "Plugging in the values, we have:\n",
    "P(T = 1) = (0.05 * 0.02) + (0.01 * 0.98)\n",
    "\n",
    "Now we can calculate P(D = 1|T = 1):\n",
    "P(D = 1|T = 1) = (0.05 * 0.02) / [(0.05 * 0.02) + (0.01 * 0.98)]\n",
    "\n",
    "Simplifying the expression:\n",
    "P(D = 1|T = 1) = 0.001 / (0.001 + 0.0098)\n",
    "\n",
    "Calculating the result:\n",
    "P(D = 1|T = 1) ≈ 0.093 or 9.3%\n",
    "\n",
    "Therefore, the likelihood that a person who tests positive is actually immune (D = 1) is approximately 9.3%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e5c889",
   "metadata": {},
   "source": [
    "# 8. In order to prepare for the test, a student knows that there will be one question in the exam that\n",
    "is either form A, B, or C. The chances of getting an A, B, or C on the exam are 30 percent, 20%, and\n",
    "50 percent, respectively. During the planning, the student solved 9 of 10 type A problems, 2 of 10\n",
    "type B problems, and 6 of 10 type C problems.\n",
    "\n",
    "1. What is the likelihood that the student can solve the exam problem?\n",
    "\n",
    "2. Given the student&#39;s solution, what is the likelihood that the problem was of form A?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab31e45",
   "metadata": {},
   "source": [
    "To answer the questions, we can use Bayesian probability and apply Bayes' theorem.\n",
    "\n",
    "Let's denote the following:\n",
    "A = Event that the problem is of form A.\n",
    "B = Event that the problem is of form B.\n",
    "C = Event that the problem is of form C.\n",
    "S = Event that the student can solve the problem.\n",
    "\n",
    "We are given the following probabilities:\n",
    "P(A) = 0.3 (Probability of the problem being of form A)\n",
    "P(B) = 0.2 (Probability of the problem being of form B)\n",
    "P(C) = 0.5 (Probability of the problem being of form C)\n",
    "\n",
    "We also know the student's solving capabilities:\n",
    "P(S|A) = 9/10 (Probability that the student can solve a type A problem)\n",
    "P(S|B) = 2/10 (Probability that the student can solve a type B problem)\n",
    "P(S|C) = 6/10 (Probability that the student can solve a type C problem)\n",
    "\n",
    "To calculate the likelihood that the student can solve the exam problem (P(S)), we need to consider all the possible forms of the problem and their probabilities:\n",
    "P(S) = P(S|A) * P(A) + P(S|B) * P(B) + P(S|C) * P(C)\n",
    "\n",
    "P(S) = (9/10) * (0.3) + (2/10) * (0.2) + (6/10) * (0.5)\n",
    "\n",
    "P(S) = 0.27 + 0.04 + 0.3\n",
    "\n",
    "P(S) = 0.61 or 61%\n",
    "\n",
    "Therefore, the likelihood that the student can solve the exam problem is 61%.\n",
    "\n",
    "To calculate the likelihood that the problem was of form A given the student's solution (P(A|S)), we can use Bayes' theorem:\n",
    "P(A|S) = (P(S|A) * P(A)) / P(S)\n",
    "\n",
    "P(A|S) = ((9/10) * (0.3)) / 0.61\n",
    "\n",
    "P(A|S) = 0.27 / 0.61\n",
    "\n",
    "P(A|S) ≈ 0.443 or 44.3%\n",
    "\n",
    "Therefore, given the student's solution, the likelihood that the problem was of form A is approximately 44.3%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2ef752",
   "metadata": {},
   "source": [
    "# 10. Create the conditional probability table associated with the node Won Toss in the Bayesian Belief\n",
    "network to represent the conditional independence assumptions of the Nave Bayes classifier for the\n",
    "match winning prediction problem in Section 6.4.4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44cd5f9",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaa9afc",
   "metadata": {},
   "source": [
    "To create the conditional probability table (CPT) associated with the node \"Won Toss\" in the Bayesian Belief Network for the match winning prediction problem, we need to make conditional independence assumptions based on the Naive Bayes classifier.\n",
    "\n",
    "Let's assume we have the following variables:\n",
    "Won Toss (W) - Random variable representing whether the team won the toss (W = 1) or not (W = 0).\n",
    "Weather (D) - Random variable representing the weather conditions during the match.\n",
    "Pitch Condition (P) - Random variable representing the condition of the pitch.\n",
    "Opponent Strength (O) - Random variable representing the strength of the opponent team.\n",
    "Match Result (R) - Random variable representing the match result (R = 1 if the team won, R = 0 if the team lost).\n",
    "\n",
    "The conditional independence assumption in the Naive Bayes classifier is that all the variables (D, P, O) are conditionally independent given the class variable (R).\n",
    "\n",
    "Based on this assumption, the conditional probability table for \"Won Toss\" (W) can be represented as follows:\n",
    "\n",
    "W\tP(W=1)\tP(W=0)\n",
    "R=1\t0.8\t0.2\n",
    "R=0\t0.6\t0.4\n",
    "In this table, we consider the probabilities of winning the toss (W) based on the match result (R). The values in the table are hypothetical and should be determined based on the specific dataset and domain knowledge.\n",
    "\n",
    "For example, if the match result (R) is 1 (team won), there is an 80% chance of winning the toss (W=1) and a 20% chance of not winning the toss (W=0). Similarly, if the match result (R) is 0 (team lost), there is a 60% chance of winning the toss (W=1) and a 40% chance of not winning the toss (W=0).\n",
    "\n",
    "Note that this is just an example, and the actual probabilities in the CPT would need to be estimated from the data or determined based on domain knowledge and expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0b97cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
